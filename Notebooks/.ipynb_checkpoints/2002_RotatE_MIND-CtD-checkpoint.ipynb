{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d6bf3a-b341-4282-9470-beaa8f275cf3",
   "metadata": {},
   "source": [
    "# Run DistMult algorithm on MIND-CtD dataset\n",
    "## Parameters:\n",
    "* batch_size: 200\n",
    "* hidden_dimension_size: 225\n",
    "* learning_rate: 0.001\n",
    "* negative_sample_size: 100\n",
    "* `double_entity_embedding`: True for ComplEx\n",
    "* `double_relation_embedding`: True for ComplEx and RotatE\n",
    "\n",
    "\n",
    "## Code for Hyperparameter Optimization\n",
    "```python\n",
    "# Code for Generating MCtD best trials\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=\"postgresql+psycopg2://rogertu:admin@localhost/optuna_test\",\n",
    ")\n",
    "mctd_dist = optuna.load_study(study_name = 'DistMult_MIND_CtD', storage = storage)\n",
    "mctd_dist.best_trial\n",
    "```\n",
    "\n",
    "## Results\n",
    "```\n",
    "FrozenTrial(number=58, values=[0.07018471468688428], datetime_start=datetime.datetime(2022, 10, 24, 16, 22, 20, 767474), datetime_complete=datetime.datetime(2022, 10, 24, 17, 17, 57, 506841), params={'batch_size': 200, 'negative_sample_size': 100, 'hidden_dimension_size': 225, 'learning_rate': 0.009380369158284479, 'max_steps': 70000}, distributions={'batch_size': IntDistribution(high=256, log=False, low=64, step=4), 'negative_sample_size': IntDistribution(high=128, log=False, low=64, step=4), 'hidden_dimension_size': IntDistribution(high=300, log=False, low=100, step=25), 'learning_rate': FloatDistribution(high=0.01, log=True, low=0.0001, step=None), 'max_steps': IntDistribution(high=100000, log=False, low=50000, step=10000)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=1058, state=TrialState.COMPLETE, value=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757f0b5c-43a4-434b-8dcf-e4128bd9b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04cec2-4efe-4ff5-9186-8d3ae66aaaaf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n",
      "Start Training......\n",
      "2022-11-02 12:18:06,457 INFO     Model: DistMult\n",
      "2022-11-02 12:18:06,458 INFO     Data Path: data/MIND_CtD\n",
      "2022-11-02 12:18:06,458 INFO     #entity: 249605\n",
      "2022-11-02 12:18:06,458 INFO     #relation: 83\n",
      "2022-11-02 12:18:11,372 INFO     #train: 9657134\n",
      "2022-11-02 12:18:11,372 INFO     #valid: 473\n",
      "2022-11-02 12:18:11,372 INFO     #test: 511\n",
      "2022-11-02 12:18:11,819 INFO     Model Parameter Configuration:\n",
      "2022-11-02 12:18:11,820 INFO     Parameter gamma: torch.Size([1]), require_grad = False\n",
      "2022-11-02 12:18:11,820 INFO     Parameter embedding_range: torch.Size([1]), require_grad = False\n",
      "2022-11-02 12:18:11,820 INFO     Parameter entity_embedding: torch.Size([249605, 225]), require_grad = True\n",
      "2022-11-02 12:18:11,820 INFO     Parameter relation_embedding: torch.Size([83, 225]), require_grad = True\n",
      "2022-11-02 12:18:55,519 INFO     Ramdomly Initializing DistMult Model...\n",
      "2022-11-02 12:18:55,519 INFO     Start Training...\n",
      "2022-11-02 12:18:55,519 INFO     init_step = 0\n",
      "2022-11-02 12:18:55,519 INFO     batch_size = 200\n",
      "2022-11-02 12:18:55,519 INFO     negative_adversarial_sampling = 1\n",
      "2022-11-02 12:18:55,519 INFO     hidden_dim = 225\n",
      "2022-11-02 12:18:55,519 INFO     gamma = 48.000000\n",
      "2022-11-02 12:18:55,519 INFO     negative_adversarial_sampling = True\n",
      "2022-11-02 12:18:55,519 INFO     adversarial_temperature = 1.000000\n",
      "2022-11-02 12:18:55,519 INFO     learning_rate = 0\n",
      "2022-11-02 12:18:56,969 INFO     Training average positive_sample_loss at step 0: 0.688967\n",
      "2022-11-02 12:18:56,969 INFO     Training average negative_sample_loss at step 0: 0.693888\n",
      "2022-11-02 12:18:56,969 INFO     Training average loss at step 0: 0.691427\n",
      "2022-11-02 12:19:00,156 INFO     Training average positive_sample_loss at step 100: 0.693413\n",
      "2022-11-02 12:19:00,156 INFO     Training average negative_sample_loss at step 100: 0.693792\n",
      "2022-11-02 12:19:00,156 INFO     Training average loss at step 100: 0.693602\n",
      "2022-11-02 12:19:02,737 INFO     Training average positive_sample_loss at step 200: 0.693186\n",
      "2022-11-02 12:19:02,737 INFO     Training average negative_sample_loss at step 200: 0.693767\n",
      "2022-11-02 12:19:02,737 INFO     Training average loss at step 200: 0.693476\n",
      "2022-11-02 12:19:05,318 INFO     Training average positive_sample_loss at step 300: 0.693426\n",
      "2022-11-02 12:19:05,318 INFO     Training average negative_sample_loss at step 300: 0.693806\n",
      "2022-11-02 12:19:05,318 INFO     Training average loss at step 300: 0.693616\n",
      "2022-11-02 12:19:07,914 INFO     Training average positive_sample_loss at step 400: 0.693096\n",
      "2022-11-02 12:19:07,914 INFO     Training average negative_sample_loss at step 400: 0.693760\n",
      "2022-11-02 12:19:07,914 INFO     Training average loss at step 400: 0.693428\n",
      "2022-11-02 12:19:10,498 INFO     Training average positive_sample_loss at step 500: 0.693339\n",
      "2022-11-02 12:19:10,498 INFO     Training average negative_sample_loss at step 500: 0.693770\n",
      "2022-11-02 12:19:10,498 INFO     Training average loss at step 500: 0.693554\n",
      "2022-11-02 12:19:13,062 INFO     Training average positive_sample_loss at step 600: 0.692714\n",
      "2022-11-02 12:19:13,062 INFO     Training average negative_sample_loss at step 600: 0.693797\n",
      "2022-11-02 12:19:13,062 INFO     Training average loss at step 600: 0.693256\n",
      "2022-11-02 12:19:15,644 INFO     Training average positive_sample_loss at step 700: 0.693096\n",
      "2022-11-02 12:19:15,644 INFO     Training average negative_sample_loss at step 700: 0.693731\n",
      "2022-11-02 12:19:15,644 INFO     Training average loss at step 700: 0.693414\n",
      "2022-11-02 12:19:18,206 INFO     Training average positive_sample_loss at step 800: 0.693562\n",
      "2022-11-02 12:19:18,207 INFO     Training average negative_sample_loss at step 800: 0.693777\n",
      "2022-11-02 12:19:18,207 INFO     Training average loss at step 800: 0.693670\n",
      "2022-11-02 12:19:20,776 INFO     Training average positive_sample_loss at step 900: 0.692897\n",
      "2022-11-02 12:19:20,776 INFO     Training average negative_sample_loss at step 900: 0.693758\n",
      "2022-11-02 12:19:20,776 INFO     Training average loss at step 900: 0.693328\n",
      "2022-11-02 12:19:23,354 INFO     Training average positive_sample_loss at step 1000: 0.693037\n",
      "2022-11-02 12:19:23,354 INFO     Training average negative_sample_loss at step 1000: 0.693749\n",
      "2022-11-02 12:19:23,354 INFO     Training average loss at step 1000: 0.693393\n",
      "2022-11-02 12:19:25,960 INFO     Training average positive_sample_loss at step 1100: 0.693034\n",
      "2022-11-02 12:19:25,960 INFO     Training average negative_sample_loss at step 1100: 0.693747\n",
      "2022-11-02 12:19:25,961 INFO     Training average loss at step 1100: 0.693390\n",
      "2022-11-02 12:19:28,538 INFO     Training average positive_sample_loss at step 1200: 0.693063\n",
      "2022-11-02 12:19:28,538 INFO     Training average negative_sample_loss at step 1200: 0.693783\n",
      "2022-11-02 12:19:28,538 INFO     Training average loss at step 1200: 0.693423\n",
      "2022-11-02 12:19:30,938 INFO     Training average positive_sample_loss at step 1300: 0.692806\n",
      "2022-11-02 12:19:30,938 INFO     Training average negative_sample_loss at step 1300: 0.693801\n",
      "2022-11-02 12:19:30,938 INFO     Training average loss at step 1300: 0.693303\n",
      "2022-11-02 12:19:32,727 INFO     Training average positive_sample_loss at step 1400: 0.692716\n",
      "2022-11-02 12:19:32,727 INFO     Training average negative_sample_loss at step 1400: 0.693762\n",
      "2022-11-02 12:19:32,727 INFO     Training average loss at step 1400: 0.693239\n",
      "2022-11-02 12:19:35,305 INFO     Training average positive_sample_loss at step 1500: 0.692804\n",
      "2022-11-02 12:19:35,305 INFO     Training average negative_sample_loss at step 1500: 0.693776\n",
      "2022-11-02 12:19:35,305 INFO     Training average loss at step 1500: 0.693290\n",
      "2022-11-02 12:19:37,170 INFO     Training average positive_sample_loss at step 1600: 0.692740\n",
      "2022-11-02 12:19:37,170 INFO     Training average negative_sample_loss at step 1600: 0.693783\n",
      "2022-11-02 12:19:37,170 INFO     Training average loss at step 1600: 0.693261\n",
      "2022-11-02 12:19:38,900 INFO     Training average positive_sample_loss at step 1700: 0.692603\n",
      "2022-11-02 12:19:38,900 INFO     Training average negative_sample_loss at step 1700: 0.693812\n",
      "2022-11-02 12:19:38,900 INFO     Training average loss at step 1700: 0.693207\n",
      "2022-11-02 12:19:41,099 INFO     Training average positive_sample_loss at step 1800: 0.692577\n",
      "2022-11-02 12:19:41,100 INFO     Training average negative_sample_loss at step 1800: 0.693769\n",
      "2022-11-02 12:19:41,100 INFO     Training average loss at step 1800: 0.693173\n",
      "2022-11-02 12:19:43,695 INFO     Training average positive_sample_loss at step 1900: 0.692471\n",
      "2022-11-02 12:19:43,696 INFO     Training average negative_sample_loss at step 1900: 0.693802\n",
      "2022-11-02 12:19:43,696 INFO     Training average loss at step 1900: 0.693137\n",
      "2022-11-02 12:19:46,282 INFO     Training average positive_sample_loss at step 2000: 0.692560\n",
      "2022-11-02 12:19:46,282 INFO     Training average negative_sample_loss at step 2000: 0.693788\n",
      "2022-11-02 12:19:46,282 INFO     Training average loss at step 2000: 0.693174\n",
      "2022-11-02 12:19:48,859 INFO     Training average positive_sample_loss at step 2100: 0.692134\n",
      "2022-11-02 12:19:48,859 INFO     Training average negative_sample_loss at step 2100: 0.693787\n",
      "2022-11-02 12:19:48,859 INFO     Training average loss at step 2100: 0.692961\n",
      "2022-11-02 12:19:51,425 INFO     Training average positive_sample_loss at step 2200: 0.692299\n",
      "2022-11-02 12:19:51,425 INFO     Training average negative_sample_loss at step 2200: 0.693801\n",
      "2022-11-02 12:19:51,425 INFO     Training average loss at step 2200: 0.693050\n",
      "2022-11-02 12:19:53,993 INFO     Training average positive_sample_loss at step 2300: 0.692256\n",
      "2022-11-02 12:19:53,993 INFO     Training average negative_sample_loss at step 2300: 0.693796\n",
      "2022-11-02 12:19:53,993 INFO     Training average loss at step 2300: 0.693026\n",
      "2022-11-02 12:19:56,567 INFO     Training average positive_sample_loss at step 2400: 0.692197\n",
      "2022-11-02 12:19:56,567 INFO     Training average negative_sample_loss at step 2400: 0.693819\n",
      "2022-11-02 12:19:56,568 INFO     Training average loss at step 2400: 0.693008\n",
      "2022-11-02 12:19:59,145 INFO     Training average positive_sample_loss at step 2500: 0.691931\n",
      "2022-11-02 12:19:59,145 INFO     Training average negative_sample_loss at step 2500: 0.693822\n",
      "2022-11-02 12:19:59,145 INFO     Training average loss at step 2500: 0.692877\n",
      "2022-11-02 12:20:01,717 INFO     Training average positive_sample_loss at step 2600: 0.691141\n",
      "2022-11-02 12:20:01,717 INFO     Training average negative_sample_loss at step 2600: 0.693838\n",
      "2022-11-02 12:20:01,717 INFO     Training average loss at step 2600: 0.692490\n",
      "2022-11-02 12:20:04,285 INFO     Training average positive_sample_loss at step 2700: 0.690967\n",
      "2022-11-02 12:20:04,285 INFO     Training average negative_sample_loss at step 2700: 0.693829\n",
      "2022-11-02 12:20:04,285 INFO     Training average loss at step 2700: 0.692398\n",
      "2022-11-02 12:20:06,859 INFO     Training average positive_sample_loss at step 2800: 0.690653\n",
      "2022-11-02 12:20:06,860 INFO     Training average negative_sample_loss at step 2800: 0.693840\n",
      "2022-11-02 12:20:06,860 INFO     Training average loss at step 2800: 0.692246\n",
      "2022-11-02 12:20:09,427 INFO     Training average positive_sample_loss at step 2900: 0.690144\n",
      "2022-11-02 12:20:09,427 INFO     Training average negative_sample_loss at step 2900: 0.693844\n",
      "2022-11-02 12:20:09,427 INFO     Training average loss at step 2900: 0.691994\n",
      "2022-11-02 12:20:11,984 INFO     Training average positive_sample_loss at step 3000: 0.689604\n",
      "2022-11-02 12:20:11,984 INFO     Training average negative_sample_loss at step 3000: 0.693847\n",
      "2022-11-02 12:20:11,984 INFO     Training average loss at step 3000: 0.691725\n",
      "2022-11-02 12:20:14,562 INFO     Training average positive_sample_loss at step 3100: 0.688567\n",
      "2022-11-02 12:20:14,562 INFO     Training average negative_sample_loss at step 3100: 0.693814\n",
      "2022-11-02 12:20:14,562 INFO     Training average loss at step 3100: 0.691190\n",
      "2022-11-02 12:20:17,117 INFO     Training average positive_sample_loss at step 3200: 0.687974\n",
      "2022-11-02 12:20:17,117 INFO     Training average negative_sample_loss at step 3200: 0.693752\n",
      "2022-11-02 12:20:17,117 INFO     Training average loss at step 3200: 0.690863\n",
      "2022-11-02 12:20:19,669 INFO     Training average positive_sample_loss at step 3300: 0.686155\n",
      "2022-11-02 12:20:19,669 INFO     Training average negative_sample_loss at step 3300: 0.693684\n",
      "2022-11-02 12:20:19,669 INFO     Training average loss at step 3300: 0.689920\n",
      "2022-11-02 12:20:22,233 INFO     Training average positive_sample_loss at step 3400: 0.684462\n",
      "2022-11-02 12:20:22,233 INFO     Training average negative_sample_loss at step 3400: 0.693459\n",
      "2022-11-02 12:20:22,233 INFO     Training average loss at step 3400: 0.688961\n",
      "2022-11-02 12:20:24,822 INFO     Training average positive_sample_loss at step 3500: 0.681718\n",
      "2022-11-02 12:20:24,822 INFO     Training average negative_sample_loss at step 3500: 0.693288\n",
      "2022-11-02 12:20:24,822 INFO     Training average loss at step 3500: 0.687503\n",
      "2022-11-02 12:20:27,378 INFO     Training average positive_sample_loss at step 3600: 0.678735\n",
      "2022-11-02 12:20:27,378 INFO     Training average negative_sample_loss at step 3600: 0.692848\n",
      "2022-11-02 12:20:27,378 INFO     Training average loss at step 3600: 0.685791\n",
      "2022-11-02 12:20:29,963 INFO     Training average positive_sample_loss at step 3700: 0.673953\n",
      "2022-11-02 12:20:29,963 INFO     Training average negative_sample_loss at step 3700: 0.692123\n",
      "2022-11-02 12:20:29,963 INFO     Training average loss at step 3700: 0.683038\n",
      "2022-11-02 12:20:32,514 INFO     Training average positive_sample_loss at step 3800: 0.669500\n",
      "2022-11-02 12:20:32,514 INFO     Training average negative_sample_loss at step 3800: 0.691279\n",
      "2022-11-02 12:20:32,514 INFO     Training average loss at step 3800: 0.680389\n",
      "2022-11-02 12:20:35,065 INFO     Training average positive_sample_loss at step 3900: 0.662623\n",
      "2022-11-02 12:20:35,066 INFO     Training average negative_sample_loss at step 3900: 0.689952\n",
      "2022-11-02 12:20:35,066 INFO     Training average loss at step 3900: 0.676288\n",
      "2022-11-02 12:20:37,617 INFO     Training average positive_sample_loss at step 4000: 0.654687\n",
      "2022-11-02 12:20:37,617 INFO     Training average negative_sample_loss at step 4000: 0.688672\n",
      "2022-11-02 12:20:37,617 INFO     Training average loss at step 4000: 0.671680\n",
      "2022-11-02 12:20:40,163 INFO     Training average positive_sample_loss at step 4100: 0.645698\n",
      "2022-11-02 12:20:40,163 INFO     Training average negative_sample_loss at step 4100: 0.686698\n",
      "2022-11-02 12:20:40,163 INFO     Training average loss at step 4100: 0.666198\n",
      "2022-11-02 12:20:42,734 INFO     Training average positive_sample_loss at step 4200: 0.636759\n",
      "2022-11-02 12:20:42,734 INFO     Training average negative_sample_loss at step 4200: 0.684727\n",
      "2022-11-02 12:20:42,734 INFO     Training average loss at step 4200: 0.660743\n",
      "2022-11-02 12:20:45,324 INFO     Training average positive_sample_loss at step 4300: 0.625632\n",
      "2022-11-02 12:20:45,324 INFO     Training average negative_sample_loss at step 4300: 0.682111\n",
      "2022-11-02 12:20:45,324 INFO     Training average loss at step 4300: 0.653872\n",
      "2022-11-02 12:20:47,952 INFO     Training average positive_sample_loss at step 4400: 0.616682\n",
      "2022-11-02 12:20:47,952 INFO     Training average negative_sample_loss at step 4400: 0.679603\n",
      "2022-11-02 12:20:47,952 INFO     Training average loss at step 4400: 0.648142\n",
      "2022-11-02 12:20:50,555 INFO     Training average positive_sample_loss at step 4500: 0.603962\n",
      "2022-11-02 12:20:50,555 INFO     Training average negative_sample_loss at step 4500: 0.676896\n",
      "2022-11-02 12:20:50,555 INFO     Training average loss at step 4500: 0.640429\n",
      "2022-11-02 12:20:53,171 INFO     Training average positive_sample_loss at step 4600: 0.591997\n",
      "2022-11-02 12:20:53,171 INFO     Training average negative_sample_loss at step 4600: 0.674394\n",
      "2022-11-02 12:20:53,171 INFO     Training average loss at step 4600: 0.633196\n",
      "2022-11-02 12:20:55,789 INFO     Training average positive_sample_loss at step 4700: 0.582557\n",
      "2022-11-02 12:20:55,789 INFO     Training average negative_sample_loss at step 4700: 0.672466\n",
      "2022-11-02 12:20:55,789 INFO     Training average loss at step 4700: 0.627511\n",
      "2022-11-02 12:20:58,370 INFO     Training average positive_sample_loss at step 4800: 0.574994\n",
      "2022-11-02 12:20:58,370 INFO     Training average negative_sample_loss at step 4800: 0.670785\n",
      "2022-11-02 12:20:58,370 INFO     Training average loss at step 4800: 0.622890\n",
      "2022-11-02 12:21:00,952 INFO     Training average positive_sample_loss at step 4900: 0.565470\n",
      "2022-11-02 12:21:00,952 INFO     Training average negative_sample_loss at step 4900: 0.669887\n",
      "2022-11-02 12:21:00,952 INFO     Training average loss at step 4900: 0.617678\n",
      "2022-11-02 12:21:03,503 INFO     Training average positive_sample_loss at step 5000: 0.553927\n",
      "2022-11-02 12:21:03,504 INFO     Training average negative_sample_loss at step 5000: 0.667516\n",
      "2022-11-02 12:21:03,504 INFO     Training average loss at step 5000: 0.610721\n",
      "2022-11-02 12:21:06,070 INFO     Training average positive_sample_loss at step 5100: 0.541221\n",
      "2022-11-02 12:21:06,070 INFO     Training average negative_sample_loss at step 5100: 0.667071\n",
      "2022-11-02 12:21:06,070 INFO     Training average loss at step 5100: 0.604146\n",
      "2022-11-02 12:21:08,604 INFO     Training average positive_sample_loss at step 5200: 0.536965\n",
      "2022-11-02 12:21:08,604 INFO     Training average negative_sample_loss at step 5200: 0.666996\n",
      "2022-11-02 12:21:08,604 INFO     Training average loss at step 5200: 0.601981\n",
      "2022-11-02 12:21:11,199 INFO     Training average positive_sample_loss at step 5300: 0.527370\n",
      "2022-11-02 12:21:11,199 INFO     Training average negative_sample_loss at step 5300: 0.666573\n",
      "2022-11-02 12:21:11,199 INFO     Training average loss at step 5300: 0.596971\n",
      "2022-11-02 12:21:13,813 INFO     Training average positive_sample_loss at step 5400: 0.522622\n",
      "2022-11-02 12:21:13,813 INFO     Training average negative_sample_loss at step 5400: 0.666980\n",
      "2022-11-02 12:21:13,813 INFO     Training average loss at step 5400: 0.594801\n",
      "2022-11-02 12:21:16,416 INFO     Training average positive_sample_loss at step 5500: 0.513770\n",
      "2022-11-02 12:21:16,416 INFO     Training average negative_sample_loss at step 5500: 0.666224\n",
      "2022-11-02 12:21:16,416 INFO     Training average loss at step 5500: 0.589997\n",
      "2022-11-02 12:21:18,977 INFO     Training average positive_sample_loss at step 5600: 0.505310\n",
      "2022-11-02 12:21:18,977 INFO     Training average negative_sample_loss at step 5600: 0.663348\n",
      "2022-11-02 12:21:18,977 INFO     Training average loss at step 5600: 0.584329\n",
      "2022-11-02 12:21:21,555 INFO     Training average positive_sample_loss at step 5700: 0.505440\n",
      "2022-11-02 12:21:21,555 INFO     Training average negative_sample_loss at step 5700: 0.662064\n",
      "2022-11-02 12:21:21,555 INFO     Training average loss at step 5700: 0.583752\n",
      "2022-11-02 12:21:24,135 INFO     Training average positive_sample_loss at step 5800: 0.493733\n",
      "2022-11-02 12:21:24,135 INFO     Training average negative_sample_loss at step 5800: 0.659450\n",
      "2022-11-02 12:21:24,135 INFO     Training average loss at step 5800: 0.576591\n",
      "2022-11-02 12:21:26,698 INFO     Training average positive_sample_loss at step 5900: 0.490432\n",
      "2022-11-02 12:21:26,698 INFO     Training average negative_sample_loss at step 5900: 0.657772\n",
      "2022-11-02 12:21:26,698 INFO     Training average loss at step 5900: 0.574102\n",
      "2022-11-02 12:21:29,265 INFO     Training average positive_sample_loss at step 6000: 0.487598\n",
      "2022-11-02 12:21:29,265 INFO     Training average negative_sample_loss at step 6000: 0.657830\n",
      "2022-11-02 12:21:29,265 INFO     Training average loss at step 6000: 0.572714\n",
      "2022-11-02 12:21:31,809 INFO     Training average positive_sample_loss at step 6100: 0.488396\n",
      "2022-11-02 12:21:31,809 INFO     Training average negative_sample_loss at step 6100: 0.650502\n",
      "2022-11-02 12:21:31,809 INFO     Training average loss at step 6100: 0.569449\n",
      "2022-11-02 12:21:34,334 INFO     Training average positive_sample_loss at step 6200: 0.487705\n",
      "2022-11-02 12:21:34,334 INFO     Training average negative_sample_loss at step 6200: 0.650449\n",
      "2022-11-02 12:21:34,334 INFO     Training average loss at step 6200: 0.569077\n",
      "2022-11-02 12:21:36,862 INFO     Training average positive_sample_loss at step 6300: 0.482477\n",
      "2022-11-02 12:21:36,862 INFO     Training average negative_sample_loss at step 6300: 0.649877\n",
      "2022-11-02 12:21:36,862 INFO     Training average loss at step 6300: 0.566177\n",
      "2022-11-02 12:21:39,395 INFO     Training average positive_sample_loss at step 6400: 0.480619\n",
      "2022-11-02 12:21:39,395 INFO     Training average negative_sample_loss at step 6400: 0.644364\n",
      "2022-11-02 12:21:39,395 INFO     Training average loss at step 6400: 0.562491\n",
      "2022-11-02 12:21:41,922 INFO     Training average positive_sample_loss at step 6500: 0.477838\n",
      "2022-11-02 12:21:41,922 INFO     Training average negative_sample_loss at step 6500: 0.641629\n",
      "2022-11-02 12:21:41,922 INFO     Training average loss at step 6500: 0.559734\n",
      "2022-11-02 12:21:44,440 INFO     Training average positive_sample_loss at step 6600: 0.474021\n",
      "2022-11-02 12:21:44,441 INFO     Training average negative_sample_loss at step 6600: 0.637432\n",
      "2022-11-02 12:21:44,441 INFO     Training average loss at step 6600: 0.555726\n",
      "2022-11-02 12:21:46,984 INFO     Training average positive_sample_loss at step 6700: 0.477077\n",
      "2022-11-02 12:21:46,985 INFO     Training average negative_sample_loss at step 6700: 0.631436\n",
      "2022-11-02 12:21:46,985 INFO     Training average loss at step 6700: 0.554257\n",
      "2022-11-02 12:21:49,510 INFO     Training average positive_sample_loss at step 6800: 0.471479\n",
      "2022-11-02 12:21:49,511 INFO     Training average negative_sample_loss at step 6800: 0.630495\n",
      "2022-11-02 12:21:49,511 INFO     Training average loss at step 6800: 0.550987\n",
      "2022-11-02 12:21:52,026 INFO     Training average positive_sample_loss at step 6900: 0.475822\n",
      "2022-11-02 12:21:52,026 INFO     Training average negative_sample_loss at step 6900: 0.625286\n",
      "2022-11-02 12:21:52,027 INFO     Training average loss at step 6900: 0.550554\n",
      "2022-11-02 12:21:54,599 INFO     Training average positive_sample_loss at step 7000: 0.480735\n",
      "2022-11-02 12:21:54,599 INFO     Training average negative_sample_loss at step 7000: 0.620689\n",
      "2022-11-02 12:21:54,599 INFO     Training average loss at step 7000: 0.550712\n",
      "2022-11-02 12:21:57,267 INFO     Training average positive_sample_loss at step 7100: 0.476663\n",
      "2022-11-02 12:21:57,267 INFO     Training average negative_sample_loss at step 7100: 0.616077\n",
      "2022-11-02 12:21:57,267 INFO     Training average loss at step 7100: 0.546370\n",
      "2022-11-02 12:21:59,876 INFO     Training average positive_sample_loss at step 7200: 0.476780\n",
      "2022-11-02 12:21:59,876 INFO     Training average negative_sample_loss at step 7200: 0.612035\n",
      "2022-11-02 12:21:59,876 INFO     Training average loss at step 7200: 0.544408\n",
      "2022-11-02 12:22:02,406 INFO     Training average positive_sample_loss at step 7300: 0.478469\n",
      "2022-11-02 12:22:02,406 INFO     Training average negative_sample_loss at step 7300: 0.604181\n",
      "2022-11-02 12:22:02,406 INFO     Training average loss at step 7300: 0.541325\n",
      "2022-11-02 12:22:04,955 INFO     Training average positive_sample_loss at step 7400: 0.480206\n",
      "2022-11-02 12:22:04,955 INFO     Training average negative_sample_loss at step 7400: 0.603429\n",
      "2022-11-02 12:22:04,955 INFO     Training average loss at step 7400: 0.541818\n",
      "2022-11-02 12:22:07,628 INFO     Training average positive_sample_loss at step 7500: 0.477868\n",
      "2022-11-02 12:22:07,628 INFO     Training average negative_sample_loss at step 7500: 0.594267\n",
      "2022-11-02 12:22:07,628 INFO     Training average loss at step 7500: 0.536068\n",
      "2022-11-02 12:22:10,298 INFO     Training average positive_sample_loss at step 7600: 0.481173\n",
      "2022-11-02 12:22:10,298 INFO     Training average negative_sample_loss at step 7600: 0.593699\n",
      "2022-11-02 12:22:10,298 INFO     Training average loss at step 7600: 0.537436\n",
      "2022-11-02 12:22:12,858 INFO     Training average positive_sample_loss at step 7700: 0.476253\n",
      "2022-11-02 12:22:12,858 INFO     Training average negative_sample_loss at step 7700: 0.590610\n",
      "2022-11-02 12:22:12,858 INFO     Training average loss at step 7700: 0.533431\n",
      "2022-11-02 12:22:15,403 INFO     Training average positive_sample_loss at step 7800: 0.481466\n",
      "2022-11-02 12:22:15,403 INFO     Training average negative_sample_loss at step 7800: 0.584094\n",
      "2022-11-02 12:22:15,403 INFO     Training average loss at step 7800: 0.532780\n",
      "2022-11-02 12:22:17,965 INFO     Training average positive_sample_loss at step 7900: 0.481889\n",
      "2022-11-02 12:22:17,965 INFO     Training average negative_sample_loss at step 7900: 0.580759\n",
      "2022-11-02 12:22:17,965 INFO     Training average loss at step 7900: 0.531324\n",
      "2022-11-02 12:22:20,514 INFO     Training average positive_sample_loss at step 8000: 0.476262\n",
      "2022-11-02 12:22:20,514 INFO     Training average negative_sample_loss at step 8000: 0.577607\n",
      "2022-11-02 12:22:20,514 INFO     Training average loss at step 8000: 0.526934\n",
      "2022-11-02 12:22:23,047 INFO     Training average positive_sample_loss at step 8100: 0.481670\n",
      "2022-11-02 12:22:23,047 INFO     Training average negative_sample_loss at step 8100: 0.573990\n",
      "2022-11-02 12:22:23,047 INFO     Training average loss at step 8100: 0.527830\n",
      "2022-11-02 12:22:25,622 INFO     Training average positive_sample_loss at step 8200: 0.492127\n",
      "2022-11-02 12:22:25,622 INFO     Training average negative_sample_loss at step 8200: 0.571064\n",
      "2022-11-02 12:22:25,622 INFO     Training average loss at step 8200: 0.531595\n",
      "2022-11-02 12:22:28,024 INFO     Training average positive_sample_loss at step 8300: 0.482121\n",
      "2022-11-02 12:22:28,024 INFO     Training average negative_sample_loss at step 8300: 0.565738\n",
      "2022-11-02 12:22:28,024 INFO     Training average loss at step 8300: 0.523930\n",
      "2022-11-02 12:22:29,831 INFO     Training average positive_sample_loss at step 8400: 0.480156\n",
      "2022-11-02 12:22:29,831 INFO     Training average negative_sample_loss at step 8400: 0.564804\n",
      "2022-11-02 12:22:29,831 INFO     Training average loss at step 8400: 0.522480\n",
      "2022-11-02 12:22:32,381 INFO     Training average positive_sample_loss at step 8500: 0.481533\n",
      "2022-11-02 12:22:32,381 INFO     Training average negative_sample_loss at step 8500: 0.560813\n",
      "2022-11-02 12:22:32,381 INFO     Training average loss at step 8500: 0.521173\n",
      "2022-11-02 12:22:34,435 INFO     Training average positive_sample_loss at step 8600: 0.482854\n",
      "2022-11-02 12:22:34,435 INFO     Training average negative_sample_loss at step 8600: 0.559203\n",
      "2022-11-02 12:22:34,435 INFO     Training average loss at step 8600: 0.521029\n",
      "2022-11-02 12:22:36,707 INFO     Training average positive_sample_loss at step 8700: 0.479182\n",
      "2022-11-02 12:22:36,707 INFO     Training average negative_sample_loss at step 8700: 0.558100\n",
      "2022-11-02 12:22:36,707 INFO     Training average loss at step 8700: 0.518641\n",
      "2022-11-02 12:22:39,069 INFO     Training average positive_sample_loss at step 8800: 0.482523\n",
      "2022-11-02 12:22:39,069 INFO     Training average negative_sample_loss at step 8800: 0.558756\n",
      "2022-11-02 12:22:39,069 INFO     Training average loss at step 8800: 0.520639\n",
      "2022-11-02 12:22:40,645 INFO     Training average positive_sample_loss at step 8900: 0.494461\n",
      "2022-11-02 12:22:40,645 INFO     Training average negative_sample_loss at step 8900: 0.552685\n",
      "2022-11-02 12:22:40,645 INFO     Training average loss at step 8900: 0.523573\n",
      "2022-11-02 12:22:43,239 INFO     Training average positive_sample_loss at step 9000: 0.484958\n",
      "2022-11-02 12:22:43,239 INFO     Training average negative_sample_loss at step 9000: 0.550184\n",
      "2022-11-02 12:22:43,239 INFO     Training average loss at step 9000: 0.517571\n",
      "2022-11-02 12:22:45,816 INFO     Training average positive_sample_loss at step 9100: 0.469923\n",
      "2022-11-02 12:22:45,817 INFO     Training average negative_sample_loss at step 9100: 0.546176\n",
      "2022-11-02 12:22:45,817 INFO     Training average loss at step 9100: 0.508050\n",
      "2022-11-02 12:22:48,368 INFO     Training average positive_sample_loss at step 9200: 0.492912\n",
      "2022-11-02 12:22:48,368 INFO     Training average negative_sample_loss at step 9200: 0.545704\n",
      "2022-11-02 12:22:48,369 INFO     Training average loss at step 9200: 0.519308\n",
      "2022-11-02 12:22:50,915 INFO     Training average positive_sample_loss at step 9300: 0.481865\n",
      "2022-11-02 12:22:50,915 INFO     Training average negative_sample_loss at step 9300: 0.543657\n",
      "2022-11-02 12:22:50,915 INFO     Training average loss at step 9300: 0.512761\n",
      "2022-11-02 12:22:53,497 INFO     Training average positive_sample_loss at step 9400: 0.476060\n",
      "2022-11-02 12:22:53,497 INFO     Training average negative_sample_loss at step 9400: 0.546786\n",
      "2022-11-02 12:22:53,497 INFO     Training average loss at step 9400: 0.511423\n",
      "2022-11-02 12:22:56,054 INFO     Training average positive_sample_loss at step 9500: 0.468737\n",
      "2022-11-02 12:22:56,054 INFO     Training average negative_sample_loss at step 9500: 0.545922\n",
      "2022-11-02 12:22:56,054 INFO     Training average loss at step 9500: 0.507329\n",
      "2022-11-02 12:22:58,636 INFO     Training average positive_sample_loss at step 9600: 0.476046\n",
      "2022-11-02 12:22:58,636 INFO     Training average negative_sample_loss at step 9600: 0.541221\n",
      "2022-11-02 12:22:58,636 INFO     Training average loss at step 9600: 0.508633\n",
      "2022-11-02 12:23:01,267 INFO     Training average positive_sample_loss at step 9700: 0.480505\n",
      "2022-11-02 12:23:01,267 INFO     Training average negative_sample_loss at step 9700: 0.535519\n",
      "2022-11-02 12:23:01,267 INFO     Training average loss at step 9700: 0.508012\n",
      "2022-11-02 12:23:03,880 INFO     Training average positive_sample_loss at step 9800: 0.470571\n",
      "2022-11-02 12:23:03,880 INFO     Training average negative_sample_loss at step 9800: 0.540639\n",
      "2022-11-02 12:23:03,880 INFO     Training average loss at step 9800: 0.505605\n",
      "2022-11-02 12:23:06,449 INFO     Training average positive_sample_loss at step 9900: 0.475198\n",
      "2022-11-02 12:23:06,449 INFO     Training average negative_sample_loss at step 9900: 0.539402\n",
      "2022-11-02 12:23:06,449 INFO     Training average loss at step 9900: 0.507300\n",
      "2022-11-02 12:23:10,330 INFO     Training average positive_sample_loss at step 10000: 0.480101\n",
      "2022-11-02 12:23:10,330 INFO     Training average negative_sample_loss at step 10000: 0.533889\n",
      "2022-11-02 12:23:10,330 INFO     Training average loss at step 10000: 0.506995\n",
      "2022-11-02 12:23:12,871 INFO     Training average positive_sample_loss at step 10100: 0.473784\n",
      "2022-11-02 12:23:12,871 INFO     Training average negative_sample_loss at step 10100: 0.537657\n",
      "2022-11-02 12:23:12,871 INFO     Training average loss at step 10100: 0.505720\n",
      "2022-11-02 12:23:15,401 INFO     Training average positive_sample_loss at step 10200: 0.470673\n",
      "2022-11-02 12:23:15,401 INFO     Training average negative_sample_loss at step 10200: 0.533435\n",
      "2022-11-02 12:23:15,401 INFO     Training average loss at step 10200: 0.502054\n",
      "2022-11-02 12:23:17,940 INFO     Training average positive_sample_loss at step 10300: 0.474062\n",
      "2022-11-02 12:23:17,940 INFO     Training average negative_sample_loss at step 10300: 0.533135\n",
      "2022-11-02 12:23:17,940 INFO     Training average loss at step 10300: 0.503599\n",
      "2022-11-02 12:23:20,536 INFO     Training average positive_sample_loss at step 10400: 0.478140\n",
      "2022-11-02 12:23:20,536 INFO     Training average negative_sample_loss at step 10400: 0.533775\n",
      "2022-11-02 12:23:20,536 INFO     Training average loss at step 10400: 0.505958\n",
      "2022-11-02 12:23:23,096 INFO     Training average positive_sample_loss at step 10500: 0.478980\n",
      "2022-11-02 12:23:23,096 INFO     Training average negative_sample_loss at step 10500: 0.529005\n",
      "2022-11-02 12:23:23,096 INFO     Training average loss at step 10500: 0.503993\n",
      "2022-11-02 12:23:25,706 INFO     Training average positive_sample_loss at step 10600: 0.466872\n",
      "2022-11-02 12:23:25,706 INFO     Training average negative_sample_loss at step 10600: 0.533781\n",
      "2022-11-02 12:23:25,706 INFO     Training average loss at step 10600: 0.500327\n",
      "2022-11-02 12:23:28,303 INFO     Training average positive_sample_loss at step 10700: 0.465571\n",
      "2022-11-02 12:23:28,303 INFO     Training average negative_sample_loss at step 10700: 0.533888\n",
      "2022-11-02 12:23:28,303 INFO     Training average loss at step 10700: 0.499729\n"
     ]
    }
   ],
   "source": [
    "# Batch Sz/ Neg_samp_Sz/ Dims/ gamma/ alpha/ lr/ steps/ test batch size/ double ent/ double rel/ regularization\n",
    "!bash run.sh train DistMult MIND_CtD 0 optimized 200 100 225 48.0 1.0 0.001 1000000 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b111e5-f3ad-4826-9f0f-5a86d748b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -u codes/run.py --do_test -init models/DistMult_MIND_CtD_optimized --cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
