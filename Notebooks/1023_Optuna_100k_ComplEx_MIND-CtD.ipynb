{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogertu/python_venvs/RotatE/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../codes/\")\n",
    "from dataloader import TestDataset, TrainDataset, BidirectionalOneShotIterator\n",
    "from run import parse_args\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# L119 in run.py\n",
    "def read_triple(file_path, entity2id, relation2id):\n",
    "    \"\"\"\n",
    "    Read triples and map them into ids.\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    with open(file_path) as fin:\n",
    "        for line in fin:\n",
    "            h, r, t = line.strip().split(\"\\t\")\n",
    "            triples.append((entity2id[h], relation2id[r], entity2id[t]))\n",
    "    return triples\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, save_variable_list, args):\n",
    "    \"\"\"\n",
    "    Save the parameters of the model and the optimizer,\n",
    "    as well as some other variables such as step and learning_rate\n",
    "    \"\"\"\n",
    "\n",
    "    argparse_dict = vars(args)\n",
    "    with open(os.path.join(args.save_path, \"config.json\"), \"w\") as fjson:\n",
    "        json.dump(argparse_dict, fjson)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            **save_variable_list,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        os.path.join(args.save_path, \"checkpoint\"),\n",
    "    )\n",
    "\n",
    "    entity_embedding = model.entity_embedding.detach().cpu().numpy()\n",
    "    np.save(os.path.join(args.save_path, \"entity_embedding\"), entity_embedding)\n",
    "\n",
    "    relation_embedding = model.relation_embedding.detach().cpu().numpy()\n",
    "    np.save(os.path.join(args.save_path, \"relation_embedding\"), relation_embedding)\n",
    "\n",
    "\n",
    "def set_logger(args):\n",
    "    \"\"\"\n",
    "    Write logs to checkpoint and console\n",
    "    \"\"\"\n",
    "\n",
    "    if args.do_train:\n",
    "        log_file = os.path.join(args.save_path or args.init_checkpoint, \"train.log\")\n",
    "    else:\n",
    "        log_file = os.path.join(args.save_path or args.init_checkpoint, \"test.log\")\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "        level=logging.INFO,\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        filename=log_file,\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(asctime)s %(levelname)-8s %(message)s\")\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger(\"\").addHandler(console)\n",
    "\n",
    "\n",
    "def log_metrics(mode, step, metrics):\n",
    "    \"\"\"\n",
    "    Print the evaluation logs\n",
    "    \"\"\"\n",
    "    for metric in metrics:\n",
    "        logging.info(\"%s %s at step %d: %f\" % (mode, metric, step, metrics[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.data_path = \"/home/rogertu/projects/KnowledgeGraphEmbedding00/data/MIND_CtD/\"\n",
    "# more parameters\n",
    "args.uni_weight = True\n",
    "args.cpu_num = 10\n",
    "args.cuda = True\n",
    "args.model = \"ComplEx\"\n",
    "args.save_path = os.path.join(\n",
    "    \"/home/rogertu/projects/KnowledgeGraphEmbedding00/models/\",\n",
    "    args.model,\n",
    ")\n",
    "args.negative_adversarial_sampling = True\n",
    "args.gamma = 48  # controls embedding range.  (gamma+2)/hidden_dim\n",
    "# should range from +/-1 to 0.1667\n",
    "args.regularization = 0.00001  # controls loss, have it off for default, on for DistMult and ComplEx\n",
    "args.save_checkpoint_steps = 10000\n",
    "args.valid_steps = 100000\n",
    "args.do_valid = True\n",
    "args.log_steps = 100\n",
    "args.test_log_steps = 10000\n",
    "\n",
    "args.double_entity_embedding = True  #  True for ComplEx  and RotatE\n",
    "args.double_relation_embedding = True  # True for ComplEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if args.save_path and not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "with open(os.path.join(args.data_path, \"entities.dict\")) as fin:\n",
    "    entity2id = dict()\n",
    "    for line in fin:\n",
    "        eid, entity = line.strip().split(\"\\t\")\n",
    "        entity2id[entity] = int(eid)\n",
    "\n",
    "with open(os.path.join(args.data_path, \"relations.dict\")) as fin:\n",
    "    relation2id = dict()\n",
    "    for line in fin:\n",
    "        rid, relation = line.strip().split(\"\\t\")\n",
    "        relation2id[relation] = int(rid)\n",
    "\n",
    "nentity = len(entity2id)\n",
    "nrelation = len(relation2id)\n",
    "\n",
    "args.nentity = nentity\n",
    "args.nrelation = nrelation\n",
    "\n",
    "train_triples = read_triple(\n",
    "    os.path.join(args.data_path, \"train.txt\"), entity2id, relation2id\n",
    ")\n",
    "valid_triples = read_triple(\n",
    "    os.path.join(args.data_path, \"valid.txt\"), entity2id, relation2id\n",
    ")\n",
    "test_triples = read_triple(\n",
    "    os.path.join(args.data_path, \"test.txt\"), entity2id, relation2id\n",
    ")\n",
    "\n",
    "all_true_triples = train_triples + valid_triples + test_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.adversarial_temperature = 1.0  # default. Modulates negative effect on loss.\n",
    "args.test_batch_size = 4\n",
    "args.warm_up_steps = None\n",
    "args.countries = None\n",
    "args.optimizer = \"adam\"\n",
    "args.do_test = False\n",
    "args.do_predict = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    \"\"\"Set items to be optimized in this\"\"\"\n",
    "\n",
    "    # varied\n",
    "    args.batch_size = trial.suggest_int(f\"batch_size\", 64, 256, step=4)\n",
    "    args.negative_sample_size = trial.suggest_int(\n",
    "        f\"negative_sample_size\", 64, 128, step=4\n",
    "    )\n",
    "    args.hidden_dim = trial.suggest_int(f\"hidden_dimension_size\", 100, 300, step=25)\n",
    "    args.learning_rate = trial.suggest_float(f\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    args.max_steps = trial.suggest_int(\"max_steps\", 50000, 100000, step=10000)\n",
    "\n",
    "    # constant var can't be referenced outside of function?\n",
    "    # if args.warm_up_steps:\n",
    "    #    warm_up_steps = args.warm_up_steps\n",
    "    # else:\n",
    "    #    warm_up_steps = args.max_steps // 2\n",
    "\n",
    "    # model\n",
    "    kge_model = model.KGEModel(\n",
    "        model_name=args.model,\n",
    "        nentity=args.nentity,\n",
    "        nrelation=args.nrelation,\n",
    "        hidden_dim=args.hidden_dim,\n",
    "        gamma=args.gamma,\n",
    "        double_entity_embedding=args.double_entity_embedding,\n",
    "        double_relation_embedding=args.double_relation_embedding,\n",
    "    )\n",
    "\n",
    "    kge_model.cuda()\n",
    "\n",
    "    # load train data\n",
    "    train_dataloader_head = DataLoader(\n",
    "        TrainDataset(\n",
    "            train_triples, nentity, nrelation, args.negative_sample_size, \"head-batch\"\n",
    "        ),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=max(1, args.cpu_num // 2),\n",
    "        collate_fn=TrainDataset.collate_fn,\n",
    "    )\n",
    "\n",
    "    train_dataloader_tail = DataLoader(\n",
    "        TrainDataset(\n",
    "            train_triples, nentity, nrelation, args.negative_sample_size, \"tail-batch\"\n",
    "        ),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=max(1, args.cpu_num // 2),\n",
    "        collate_fn=TrainDataset.collate_fn,\n",
    "    )\n",
    "    # every other sample will be head or tail\n",
    "    train_iterator = BidirectionalOneShotIterator(\n",
    "        train_dataloader_head, train_dataloader_tail\n",
    "    )\n",
    "\n",
    "    # Set training configuration and optimizer\n",
    "    current_learning_rate = args.learning_rate\n",
    "    if args.optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
    "            lr=current_learning_rate,\n",
    "        )\n",
    "    elif args.optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
    "            lr=current_learning_rate,\n",
    "        )\n",
    "    elif args.optimizer == \"adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(\n",
    "            filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
    "            lr=current_learning_rate,\n",
    "        )\n",
    "\n",
    "    # add in optimizer for steps otherwise LR doesn't really matter\n",
    "    training_logs = []\n",
    "    for step in range(0, args.max_steps):\n",
    "        log = kge_model.train_step(kge_model, optimizer, train_iterator, args)\n",
    "\n",
    "        training_logs.append(log)\n",
    "\n",
    "        if step % args.save_checkpoint_steps == 0:\n",
    "            save_variable_list = {\n",
    "                \"step\": step,\n",
    "                \"current_learning_rate\": current_learning_rate,\n",
    "                #'warm_up_steps': warm_up_steps\n",
    "            }\n",
    "            save_model(kge_model, optimizer, save_variable_list, args)\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            metrics = {}\n",
    "            for metric in training_logs[0].keys():\n",
    "                metrics[metric] = sum([log[metric] for log in training_logs]) / len(\n",
    "                    training_logs\n",
    "                )\n",
    "            log_metrics(\"Training average\", step, metrics)\n",
    "            training_logs = []\n",
    "\n",
    "        if args.do_valid and (step / (args.max_steps - 1) == 1):\n",
    "            logging.info(\"Evaluating on Valid Dataset...\")\n",
    "            metrics = kge_model.test_step(\n",
    "                kge_model, valid_triples, all_true_triples, args\n",
    "            )\n",
    "            log_metrics(\"Valid\", step, metrics)\n",
    "\n",
    "    save_variable_list = {\n",
    "        \"step\": step,\n",
    "        \"current_learning_rate\": current_learning_rate,\n",
    "        #'warm_up_steps': warm_up_steps\n",
    "    }\n",
    "    save_model(kge_model, optimizer, save_variable_list, args)\n",
    "\n",
    "    # return log['loss'] #for loss\n",
    "    torch.cuda.empty_cache() # clear memory buildup from multiple models\n",
    "    return metrics[\"MRR\"]  # MRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# other server url\n",
    "url=\"postgresql+psycopg2://root:su07dev@su07:5432/optuna_test\"\n",
    "```\n",
    "\n",
    "If you get an error to update your optuna storage, follow the code below\n",
    "```bash\n",
    "optuna storage upgrade --storage $STORAGE_URL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other server url \n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=\"postgresql+psycopg2://rogertu:admin@localhost/optuna_test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-26 23:22:43,493]\u001b[0m Using an existing study with name 'ComplEx_MIND_CtD' instead of creating a new one.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create a new study.\n",
    "study = optuna.create_study(\n",
    "    study_name=\"ComplEx_MIND_CtD\",\n",
    "    direction=\"maximize\",\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 01:14:41,559]\u001b[0m Trial 0 finished with value: 0.03858639779917029 and parameters: {'batch_size': 192, 'negative_sample_size': 100, 'hidden_dimension_size': 250, 'learning_rate': 0.0003648129556064506, 'max_steps': 60000}. Best is trial 0 with value: 0.03858639779917029.\u001b[0m\n",
      "\u001b[32m[I 2022-10-27 02:44:27,813]\u001b[0m Trial 1 finished with value: 0.0397483445549268 and parameters: {'batch_size': 144, 'negative_sample_size': 128, 'hidden_dimension_size': 225, 'learning_rate': 0.0005469666520817934, 'max_steps': 50000}. Best is trial 1 with value: 0.0397483445549268.\u001b[0m\n",
      "\u001b[32m[I 2022-10-27 04:15:27,045]\u001b[0m Trial 2 finished with value: 0.04478150149128381 and parameters: {'batch_size': 176, 'negative_sample_size': 120, 'hidden_dimension_size': 150, 'learning_rate': 0.0005439205518660703, 'max_steps': 80000}. Best is trial 2 with value: 0.04478150149128381.\u001b[0m\n",
      "\u001b[32m[I 2022-10-27 05:14:16,342]\u001b[0m Trial 3 finished with value: 0.04187085994415858 and parameters: {'batch_size': 212, 'negative_sample_size': 104, 'hidden_dimension_size': 100, 'learning_rate': 0.00038007545595721226, 'max_steps': 80000}. Best is trial 2 with value: 0.04478150149128381.\u001b[0m\n",
      "\u001b[32m[I 2022-10-27 06:31:58,702]\u001b[0m Trial 4 finished with value: 0.03392992730556561 and parameters: {'batch_size': 112, 'negative_sample_size': 104, 'hidden_dimension_size': 200, 'learning_rate': 0.0017250330085435714, 'max_steps': 60000}. Best is trial 2 with value: 0.04478150149128381.\u001b[0m\n",
      "\u001b[32m[I 2022-10-27 07:09:34,328]\u001b[0m Trial 5 finished with value: 0.035726162290140764 and parameters: {'batch_size': 136, 'negative_sample_size': 116, 'hidden_dimension_size': 100, 'learning_rate': 0.0004688915170214893, 'max_steps': 50000}. Best is trial 2 with value: 0.04478150149128381.\u001b[0m\n",
      "\u001b[32m[I 2022-10-27 08:35:06,219]\u001b[0m Trial 6 finished with value: 0.039490411873937495 and parameters: {'batch_size': 124, 'negative_sample_size': 100, 'hidden_dimension_size': 225, 'learning_rate': 0.0002618331125757531, 'max_steps': 90000}. Best is trial 2 with value: 0.04478150149128381.\u001b[0m\n",
      "\u001b[32m[I 2022-10-27 10:01:13,152]\u001b[0m Trial 7 finished with value: 0.039416693805213926 and parameters: {'batch_size': 216, 'negative_sample_size': 64, 'hidden_dimension_size': 300, 'learning_rate': 0.001738632381748151, 'max_steps': 50000}. Best is trial 2 with value: 0.04478150149128381.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study.optimize(define_model, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.importance.get_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
